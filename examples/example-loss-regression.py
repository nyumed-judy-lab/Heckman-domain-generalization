import typing
import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
def _linspace_with_grads(start: torch.Tensor, stop: torch.Tensor, steps: int, device: str = 'cpu'):
    """
    Creates a 1-dimensional grid while preserving the gradients.
    Reference:
        https://github.com/esa/torchquad/blob/4be241e8462949abcc8f1ace48d7f8e5ee7dc136/torchquad/integration/utils.py#L7
    """
    grid = torch.linspace(0, 1, steps, device=device)  # Create a 0 to 1 spaced grid
    grid *= stop - start                               # Scale to desired range, keeping gradients
    grid += start                                      # TODO; sanity check
    return grid



def bivariate_normal_cdf(a: torch.Tensor, b: torch.Tensor, rho: torch.FloatTensor, steps: int = 100):
    """
    Approximation of standard bivariate normal cdf using the trapezoid rule.
    The decomposition is based on:
        Drezner, Z., & Wesolowsky, G. O. (1990).
        On the computation of the bivariate normal integral.
        Journal of Statistical Computation and Simulation, 35(1-2), 101-107.
    Arguments:
        a: 1D tensor of shape (B, )
        b: 1D tensor of shape (B, )
        rho:
    """
    device = a.device
    a, b = a.view(-1, 1), b.view(-1, 1)  # for proper broadcasting with x
    _normal = torch.distributions.Normal(loc=0., scale=1.)

    x = _linspace_with_grads(start=0, stop=rho, steps=steps,
                             device=device).unsqueeze(0).expand(a.shape[0], -1)  # (B, steps)
    # x = _linspace_with_grads(start=0, stop=rho, steps=steps).repeat(a.shape[0], 1)
    y = 1 / torch.sqrt(1 - torch.pow(x, 2)) * torch.exp(
        - (torch.pow(a, 2) + torch.pow(b, 2) + 2 * a * b * x) / (2 * (1 - torch.pow(x, 2)))
        )

    return _normal.cdf(a.squeeze()) * _normal.cdf(b.squeeze()) + \
        (1 / (2 * np.pi)) * torch.trapezoid(y=y, x=x)



y_true = torch.Tensor([-0.4141, -0.4423, -0.4231, -0.3709, -0.6389, -0.4851, -0.7453,  1.2307,
         0.7781, -0.5603, -0.8158,  0.9695, -0.4248,  0.2862, -0.5994, -0.6289,
        -0.2710, -0.3412, -0.3820,  1.6934,  1.1691, -0.6563,  0.9012,  1.3755,
        -0.1252,  0.9266,  1.9700,  0.0652,  0.0683,  1.6306,  1.6485,  1.5211,
         1.3356, -0.1295,  1.2742,  1.0363, -0.6780,  1.3542,  1.4445, -0.6077,
         1.0043,  1.7848,  0.2557, -0.8547, -0.5936,  1.0423, -0.8503, -0.8490,
        -0.0411,  0.7494, -0.6123,  0.6448, -0.4393, -0.3358, -0.4441,  0.7802,
        -0.5187, -0.1126, -0.5077,  1.3387,  0.1774, -0.4578, -0.0456, -0.7577])
y_pred = torch.Tensor([0.5586, 0.5241, 0.4834, 0.5833, 0.5582, 0.4928, 0.5120, 0.5448, 0.6628,
        0.5500, 0.5605, 0.5658, 0.5972, 0.5473, 0.5682, 0.6496, 0.5333, 0.5329,
        0.5954, 0.5702, 0.4973, 0.5460, 0.4958, 0.5641, 0.4174, 0.5096, 0.5459,
        0.5540, 0.4886, 0.5074, 0.5639, 0.5633, 0.5833, 0.5569, 0.6658, 0.5894,
        0.4901, 0.5439, 0.5379, 0.5673, 0.5392, 0.5811, 0.5020, 0.5836, 0.5487,
        0.5595, 0.5491, 0.5274, 0.5319, 0.4686, 0.5395, 0.5523, 0.5148, 0.5263,
        0.5771, 0.6085, 0.5703, 0.5482, 0.5328, 0.5768, 0.5817, 0.5934, 0.5455,
        0.7065])
s_true = torch.Tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])
s_pred = torch.Tensor([[0.3300, 0.3800, 0.3900, 0.5300, 0.3400, 0.5200, 0.3300, 0.5300, 0.5700,
         0.5300, 0.4200, 0.5200, 0.4400],
        [0.4200, 0.4500, 0.4200, 0.5600, 0.3600, 0.4400, 0.3900, 0.5500, 0.6200,
         0.6000, 0.3200, 0.5300, 0.5300],
        [0.3600, 0.4400, 0.4600, 0.5500, 0.3800, 0.4500, 0.3900, 0.5200, 0.6500,
         0.5500, 0.3400, 0.5000, 0.4600],
        [0.3400, 0.3700, 0.4000, 0.5800, 0.3600, 0.4600, 0.3900, 0.5900, 0.6300,
         0.6100, 0.3500, 0.5700, 0.4800],
        [0.3800, 0.4100, 0.4300, 0.5500, 0.3600, 0.4300, 0.3800, 0.5200, 0.5800,
         0.5900, 0.3700, 0.5300, 0.4500],
        [0.4000, 0.3700, 0.4600, 0.5800, 0.4000, 0.4000, 0.4000, 0.5900, 0.6500,
         0.6400, 0.3300, 0.5000, 0.5400],
        [0.4800, 0.5000, 0.4500, 0.6500, 0.3700, 0.4000, 0.4000, 0.4900, 0.6700,
         0.6400, 0.2600, 0.4300, 0.4800],
        [0.3700, 0.3600, 0.4300, 0.5800, 0.4300, 0.4100, 0.4200, 0.5300, 0.6500,
         0.6300, 0.3500, 0.5300, 0.5000],
        [0.4700, 0.5500, 0.4400, 0.6100, 0.4300, 0.3600, 0.3600, 0.4800, 0.7000,
         0.5100, 0.3000, 0.5300, 0.5000],
        [0.3400, 0.3900, 0.4900, 0.5200, 0.4500, 0.4500, 0.4000, 0.5800, 0.6300,
         0.6200, 0.4300, 0.5500, 0.5300],
        [0.3500, 0.3700, 0.5100, 0.6300, 0.3500, 0.4400, 0.4200, 0.5500, 0.6700,
         0.5900, 0.3800, 0.4800, 0.4700],
        [0.3500, 0.3900, 0.4600, 0.5300, 0.3400, 0.4700, 0.3300, 0.5200, 0.5800,
         0.5500, 0.4000, 0.5200, 0.4400],
        [0.5500, 0.5800, 0.5200, 0.8300, 0.0900, 0.3000, 0.6900, 0.4100, 0.9300,
         0.3500, 0.0700, 0.2200, 0.5300],
        [0.3700, 0.3600, 0.4000, 0.5600, 0.3800, 0.4500, 0.3900, 0.5600, 0.6300,
         0.6100, 0.3700, 0.5700, 0.4900],
        [0.3100, 0.4300, 0.4300, 0.4900, 0.4200, 0.4400, 0.3100, 0.5000, 0.6100,
         0.5900, 0.4400, 0.5300, 0.5200],
        [0.4500, 0.5300, 0.5100, 0.7300, 0.3600, 0.3800, 0.3300, 0.4200, 0.7100,
         0.6600, 0.3200, 0.4800, 0.3500],
        [0.4400, 0.4500, 0.4400, 0.5500, 0.4100, 0.3900, 0.4100, 0.5600, 0.6000,
         0.6300, 0.3500, 0.4900, 0.5300],
        [0.3600, 0.4100, 0.4100, 0.5500, 0.3900, 0.4400, 0.3800, 0.5600, 0.6100,
         0.6000, 0.3800, 0.5600, 0.4900],
        [0.4300, 0.4400, 0.4700, 0.5200, 0.3300, 0.4400, 0.3500, 0.4900, 0.5900,
         0.5700, 0.3700, 0.5200, 0.4500],
        [0.3700, 0.4500, 0.4600, 0.5900, 0.3700, 0.4200, 0.4100, 0.5000, 0.6500,
         0.6000, 0.3600, 0.4800, 0.4700],
        [0.4500, 0.4100, 0.4100, 0.7100, 0.3500, 0.4600, 0.5500, 0.5500, 0.6400,
         0.6600, 0.3100, 0.4000, 0.4000],
        [0.4100, 0.4100, 0.4000, 0.5900, 0.3700, 0.4300, 0.4300, 0.5400, 0.6500,
         0.6100, 0.3600, 0.5100, 0.5400],
        [0.3400, 0.4100, 0.4500, 0.5200, 0.4000, 0.4500, 0.3800, 0.5700, 0.5700,
         0.5200, 0.3600, 0.5200, 0.4600],
        [0.3800, 0.3700, 0.4600, 0.5200, 0.3800, 0.4500, 0.3800, 0.6300, 0.5900,
         0.5900, 0.3700, 0.5400, 0.4700],
        [0.6200, 0.6700, 0.4700, 0.8900, 0.1400, 0.1500, 0.4700, 0.1400, 0.8400,
         0.5800, 0.2300, 0.2100, 0.3500],
        [0.3900, 0.5500, 0.4200, 0.5200, 0.3800, 0.4100, 0.3700, 0.6100, 0.6000,
         0.5800, 0.3600, 0.5600, 0.5400],
        [0.3400, 0.3900, 0.4400, 0.4900, 0.3400, 0.4500, 0.3400, 0.4900, 0.6100,
         0.5500, 0.4300, 0.5500, 0.4500],
        [0.3800, 0.4100, 0.4100, 0.4900, 0.4400, 0.4200, 0.3800, 0.5500, 0.6200,
         0.6200, 0.4200, 0.5700, 0.5000],
        [0.4500, 0.4900, 0.5200, 0.6300, 0.3000, 0.4100, 0.4400, 0.5100, 0.6700,
         0.5300, 0.2000, 0.5000, 0.4200],
        [0.4600, 0.3800, 0.3800, 0.5900, 0.3200, 0.4300, 0.4100, 0.5800, 0.5900,
         0.6000, 0.3600, 0.5400, 0.4900],
        [0.3900, 0.5700, 0.4400, 0.4900, 0.3600, 0.4100, 0.3300, 0.6000, 0.5500,
         0.5800, 0.3700, 0.5400, 0.5500],
        [0.3300, 0.3500, 0.4200, 0.6000, 0.3300, 0.4700, 0.4600, 0.5600, 0.6100,
         0.6200, 0.3000, 0.5000, 0.4900],
        [0.4000, 0.4100, 0.4400, 0.5300, 0.3700, 0.4200, 0.3600, 0.6000, 0.6200,
         0.5900, 0.3700, 0.5200, 0.4800],
        [0.3800, 0.3900, 0.4500, 0.5600, 0.4200, 0.4000, 0.3600, 0.6300, 0.6200,
         0.5700, 0.3600, 0.5800, 0.4700],
        [0.4300, 0.4700, 0.4800, 0.7700, 0.2800, 0.4300, 0.3800, 0.4300, 0.7700,
         0.6600, 0.2400, 0.4400, 0.4800],
        [0.3200, 0.5700, 0.6600, 0.6700, 0.4500, 0.4200, 0.3800, 0.6000, 0.6500,
         0.4400, 0.2500, 0.4500, 0.4900],
        [0.6600, 0.4000, 0.6200, 0.8500, 0.2400, 0.2900, 0.6800, 0.3300, 0.9200,
         0.4600, 0.1500, 0.5500, 0.3700],
        [0.4000, 0.4800, 0.4200, 0.5600, 0.4100, 0.4600, 0.3400, 0.5400, 0.6400,
         0.5800, 0.3800, 0.5300, 0.5100],
        [0.5400, 0.5300, 0.5700, 0.7100, 0.0900, 0.3000, 0.6200, 0.2900, 0.9100,
         0.2600, 0.0700, 0.3800, 0.4300],
        [0.3700, 0.4200, 0.4100, 0.5200, 0.4200, 0.4200, 0.3900, 0.5700, 0.6200,
         0.6300, 0.4200, 0.5100, 0.4700],
        [0.3400, 0.3300, 0.6100, 0.7200, 0.3600, 0.3500, 0.3900, 0.6800, 0.8600,
         0.6400, 0.3000, 0.5400, 0.4300],
        [0.4100, 0.6100, 0.4000, 0.6200, 0.2100, 0.3500, 0.3500, 0.4500, 0.7600,
         0.5300, 0.1700, 0.4800, 0.4200],
        [0.6000, 0.5200, 0.5400, 0.6700, 0.2900, 0.4300, 0.5000, 0.4200, 0.7100,
         0.6500, 0.2700, 0.3400, 0.4100],
        [0.3600, 0.4000, 0.4200, 0.5200, 0.4200, 0.4200, 0.3700, 0.5500, 0.6200,
         0.6000, 0.4100, 0.5600, 0.5000],
        [0.4000, 0.3800, 0.4000, 0.5800, 0.4100, 0.4400, 0.4100, 0.6300, 0.6200,
         0.6200, 0.3800, 0.5500, 0.4600],
        [0.4400, 0.4800, 0.4200, 0.5100, 0.3700, 0.4100, 0.3600, 0.5900, 0.5800,
         0.6100, 0.3700, 0.5300, 0.5100],
        [0.4000, 0.4000, 0.4100, 0.5400, 0.4100, 0.4500, 0.3800, 0.5700, 0.6100,
         0.6100, 0.3700, 0.5600, 0.5200],
        [0.3700, 0.3800, 0.3700, 0.5600, 0.3400, 0.4700, 0.3500, 0.5400, 0.5900,
         0.5300, 0.3700, 0.5300, 0.4400],
        [0.4100, 0.4200, 0.5400, 0.5800, 0.3500, 0.4300, 0.3900, 0.5800, 0.7100,
         0.5800, 0.2600, 0.5000, 0.4900],
        [0.4400, 0.4900, 0.5900, 0.6700, 0.3600, 0.3300, 0.4200, 0.5300, 0.7600,
         0.5300, 0.2300, 0.4400, 0.4000],
        [0.4500, 0.3900, 0.6000, 0.7100, 0.2100, 0.2700, 0.4300, 0.5200, 0.8400,
         0.4700, 0.1700, 0.6800, 0.4500],
        [0.3900, 0.4200, 0.4800, 0.5000, 0.4000, 0.4400, 0.3400, 0.5500, 0.6400,
         0.6100, 0.4300, 0.5600, 0.5000],
        [0.4300, 0.4500, 0.5200, 0.5800, 0.3200, 0.4700, 0.3800, 0.5300, 0.6500,
         0.6200, 0.3300, 0.5700, 0.5100],
        [0.4300, 0.4600, 0.5400, 0.6500, 0.4200, 0.4400, 0.3300, 0.6100, 0.6600,
         0.6100, 0.3000, 0.4900, 0.4300],
        [0.3700, 0.4100, 0.4200, 0.4900, 0.3700, 0.4300, 0.3700, 0.5400, 0.6200,
         0.5700, 0.4000, 0.5300, 0.4900],
        [0.4800, 0.4100, 0.6400, 0.6700, 0.2600, 0.2600, 0.4600, 0.4900, 0.8500,
         0.4800, 0.2600, 0.5000, 0.3900],
        [0.3700, 0.3900, 0.4100, 0.5600, 0.3900, 0.4600, 0.3500, 0.5600, 0.6000,
         0.5900, 0.3500, 0.5000, 0.5100],
        [0.5700, 0.5200, 0.6100, 0.6200, 0.3300, 0.3700, 0.4700, 0.5000, 0.8000,
         0.5700, 0.1700, 0.5100, 0.4900],
        [0.4100, 0.3200, 0.4300, 0.5900, 0.4500, 0.4900, 0.4300, 0.6500, 0.6500,
         0.6200, 0.3200, 0.5600, 0.5200],
        [0.5200, 0.5600, 0.6300, 0.6700, 0.1500, 0.2500, 0.4600, 0.3500, 0.7900,
         0.5500, 0.1900, 0.4100, 0.3100],
        [0.3500, 0.3700, 0.4400, 0.5300, 0.3900, 0.4500, 0.3700, 0.5800, 0.6200,
         0.5600, 0.4000, 0.5600, 0.4600],
        [0.4300, 0.5100, 0.4700, 0.5200, 0.4100, 0.5200, 0.3600, 0.5700, 0.6300,
         0.6000, 0.3100, 0.4600, 0.4200],
        [0.3900, 0.3800, 0.4200, 0.6200, 0.4000, 0.4700, 0.3900, 0.5900, 0.6600,
         0.5800, 0.3400, 0.5600, 0.5100],
        [0.4300, 0.4600, 0.5500, 0.7000, 0.2800, 0.4300, 0.3200, 0.5500, 0.7900,
         0.6200, 0.1900, 0.4700, 0.4400]])



safe_log = lambda x: torch.log(torch.clip(x, 1e-6))

'''
Equation 19 in ICLR paper
'''
"""
Arguments:
    y_pred : 1d `torch.FloatTensor` of shape (N,  ),
    y_true : 1d `torch.FloatTensor` of shape (N,  ),
    s_pred : 2d `torch.FloatTensor` of shape (N, K),
    s_true : 1d `torch.LongTensor`  of shape (N,  ),
    rho    : 1d `torch.FloatTensor` of shape (N,  ),
    sigma  : 1d `torch.FloatTensor` of shape (1,  ), singleton.
"""

loss = 0.
for k in range(self.args.num_domains):
# for k in range(args.num_domains):
    # k=0
    y_pred = batch_prob.detach().cpu()
    y_true = y.clone()
    s_true = s[:,k]
    s_pred = batch_probits_g[:,k]
    rho = network.rho[k]
    sigma = network.sigma[k]
    '''
    y_pred = batch_prob.to(args.device)
    y_true = y.to(args.device)
    s_true = s[:,k].to(args.device)
    s_pred = s[:,k].to(args.device)
    rho = network.rho[k].clone()
    sigma = network.sigma[k].clone()
    
    
    '''
    epsilon: float = 1e-5
    normal = torch.distributions.Normal(loc=0., scale=1.)
    # A) Selection loss for unselected (unlabeled) individuals: (N,  )
    # Loss takes value of zero for S=1
    loss_not_selected = - (1 - s_true) * torch.log(1 - normal.cdf(s_pred) + epsilon)

    # B) Selection loss for selected (labeled) individuals: (N,  )
    probit_loss = - torch.log(
        normal.cdf(
            (s_pred + rho * (y_true - y_pred).div(sigma)).div(torch.sqrt(1 - rho ** 2))
        ) + epsilon
    )
    
    # C) Regression loss for selected individuals: (N,  )
    regression_loss = 0.5 * (
        torch.log(2 * torch.pi * (sigma ** 2)) \
            + F.mse_loss(y_pred, y_true, reduction='none').div(sigma ** 2)
    )
    loss += (loss_not_selected + s_true * (probit_loss + regression_loss)).mean()
    

def heckman_regression_nll(y_pred: torch.FloatTensor, y_true: torch.FloatTensor,
                           s_pred: torch.FloatTensor, s_true: torch.LongTensor,
                           rho: torch.FloatTensor, sigma: torch.FloatTensor) -> torch.FloatTensor:
    """Regression loss function for Heckman models."""

    y_pred = y_pred.squeeze()  # continuous real values
    y_true = y_true.squeeze()
    s_pred = s_pred.squeeze()  # in probits
    s_true = s_true.squeeze()
    
    epsilon: float = 1e-5
    normal = torch.distributions.Normal(loc=0., scale=1.)

    # A) Selection loss for unselected (unlabeled) individuals: (N,  )
    #   Loss takes value of zero for S=1
    loss_not_selected = - (1 - s_true) * torch.log(1 - normal.cdf(s_pred) + epsilon)

    # B) Selection loss for selected (labeled) individuals: (N,  )
    probit_loss = - torch.log(
        normal.cdf(
            (s_pred + rho * (y_true - y_pred).div(sigma)).div(torch.sqrt(1 - rho ** 2))
        ) + epsilon
    )

    # C) Regression loss for selected individuals: (N,  )
    regression_loss = 0.5 * (
        torch.log(2 * torch.pi * (sigma ** 2)) \
            + F.mse_loss(y_pred, y_true, reduction='none').div(sigma ** 2)
    )

    return (loss_not_selected + s_true * (probit_loss + regression_loss)).mean()


def _cross_domain_regression_loss(self,
                                    y_pred: torch.FloatTensor,
                                    y_true: torch.FloatTensor,
                                    s_pred: torch.FloatTensor,
                                    s_true: torch.LongTensor,
                                    rho   : torch.FloatTensor,
                                    sigma : torch.FloatTensor, ) -> torch.FloatTensor:
    """
    Arguments:
        y_pred : 1d `torch.FloatTensor` of shape (N,  ),
        y_true : 1d `torch.FloatTensor` of shape (N,  ),
        s_pred : 2d `torch.FloatTensor` of shape (N, K),
        s_true : 1d `torch.LongTensor`  of shape (N,  ),
        rho    : 1d `torch.FloatTensor` of shape (N,  ),
        sigma  : 1d `torch.FloatTensor` of shape (1,  ), singleton.
    """

    if (y_pred.ndim == 2) and (y_pred.size(1) == 1):
        y_pred = y_pred.squeeze(1)
    if (y_true.ndim == 2) and (y_true.size(1) == 1):
        y_true = y_true.squeeze(1)

    _epsilon: float = 1e-7
    _normal = torch.distributions.Normal(loc=0., scale=1.)

    # Gather values from `s_pred` those that correspond to the true domains
    s_pred_k = s_pred.gather(dim=1, index=s_true.view(-1, 1)).squeeze()

    # -\log p[S_k = 1, y] = -\log p(y) -\log p(S_k = 1 | y) ; shape = (N,  )
    loss_selected = - torch.log(
        _normal.cdf(
            (s_pred_k + rho * (y_true - y_pred).div(_epsilon + sigma)) / (_epsilon + torch.sqrt(1 - rho ** 2))
        ) + _epsilon
    ) + 0.5 * (
        torch.log(2 * torch.pi * (sigma ** 2)) \
            + F.mse_loss(y_pred, y_true, reduction='none').div(_epsilon + sigma ** 2)
    )

    if (self.args.freeze_selection_encoder & self.args.freeze_selection_head):
        return loss_selected.mean()

    # domain indicators in 2d; (N, K) <- (N,  )
    s_true_2d = torch.zeros_like(s_pred).scatter_(
        dim=1,
        index=s_true.view(-1, 1),
        src=torch.ones_like(s_pred),
    )

    # -\log Pr[S_l = 0] for l \neq k
    #   shape; (N(K-1),  )
    loss_not_selected = - torch.log(1 - _normal.cdf(s_pred) + _epsilon)                    # (N     , K)
    loss_not_selected = torch.nan_to_num(loss_not_selected, nan=0., posinf=0., neginf=0.)  # (N     , K)
    loss_not_selected = loss_not_selected.masked_select((1 - s_true_2d).bool())            # (N(K-1),  )

    return torch.cat([loss_selected, loss_not_selected], dim=0).mean(), loss_selected.mean(), loss_not_selected.mean()

